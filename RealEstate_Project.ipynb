{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                   WEB SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Real Estate Property From Web Pages Using BeautifulSoup Library And Saving Cleaned Data Into CSV Format Using Pandas Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To crawl all webpages and find out the  required parameters of all properties(houses) for sale in below locations from real estate website \"Century21.com\"\n",
    "\n",
    "1) Rock Springs is a city in Sweetwater County, Wyoming, United States.\n",
    "\n",
    "2) Rocksprings is a town in Edwards County, Texas, in the United States.\n",
    "\n",
    "3) Black Canyon City is a census-designated place (CDP) in Yavapai County, Arizona, United States.\n",
    "\n",
    "**We need to extract following data from each property in above location which are for sale:**\n",
    "\n",
    "1) Street Address and house/apartment number\n",
    "\n",
    "2) city, state, and ZIP code \n",
    "\n",
    "3) Property Price\n",
    "\n",
    "4) Number of Bed rooms, full baths, half baths\n",
    "\n",
    "5) Area of property\n",
    "\n",
    "6) Lot Size\n",
    "\n",
    "**Note: Since web pages scraping is illegal we are using archived internet pages for educational purpose only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://www.pyclass.com/real-estate/rock-springs-wy/LCWYROCKSPRINGS\"\n",
    "\n",
    "# Finding the total number of search pages available\n",
    "\n",
    "page_Number = requests.get(url, \\\n",
    "                    headers={'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'})\n",
    "\n",
    "page_Number1 = BeautifulSoup(page_Number.content, \"html.parser\")\n",
    "\n",
    "Pages = int(page_Number1.find_all('a',{'class' :'Page' })[-1].text)\n",
    "\n",
    "print(\"Total search Pages:\", Pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = []\n",
    "\n",
    "for  page in range(0,Pages*10,10):\n",
    "    \n",
    "    data = requests.get(url+\"/t=0&s=\"+str(page)+\".html\", \\\n",
    "                    headers={'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'})\n",
    "\n",
    "    soup = BeautifulSoup(data.content,\"html.parser\")\n",
    "\n",
    "    mainRow = soup.find_all('div', {'class':\"propertyRow\"})\n",
    "    \n",
    "    for i in range(len(mainRow)):\n",
    "\n",
    "        details = {}\n",
    "        details['Address_street'] = (mainRow[i].find_all('span',{'class' : 'propAddressCollapse'})[0].text)\n",
    "        details['Address_locality'] = (mainRow[i].find_all('span',{'class' : 'propAddressCollapse'})[1].text)\n",
    "        details['Price'] = (mainRow[i].find('h4',{'class' : 'propPrice' }).text.replace('\\n','').strip())\n",
    "        bed = mainRow[i].find('span',{'class' : 'infoBed'})\n",
    "\n",
    "        if bed is not None:\n",
    "            details['Bed Rooms'] = (bed.find('b').text)\n",
    "        else: details['Bed Rooms'] = None\n",
    "\n",
    "        full_bath = mainRow[i].find('span',{'class' : 'infoValueFullBath'})\n",
    "        half_bath = mainRow[i].find('span',{'class' : 'infoValueHalfBath'})\n",
    "\n",
    "\n",
    "        if (full_bath is not None):\n",
    "            details['Full Baths'] = full_bath.find('b').text\n",
    "        else: details['Full Baths'] = None\n",
    "\n",
    "        if (half_bath is not None):\n",
    "            details['Half Baths'] = half_bath.find('b').text\n",
    "        else: details['Half Baths'] = None\n",
    "\n",
    "        area = mainRow[i].find('span',{'class' : 'infoSqFt'})\n",
    "\n",
    "        if area is not None:\n",
    "            details['Area'] = area.find('b').text\n",
    "        else: details['Area'] = None\n",
    "\n",
    "        getFeatures = mainRow[i].find_all('div',{'class' : 'columnGroup'})\n",
    "\n",
    "        for item in getFeatures:\n",
    "            if item.find('span',{'class' : 'featureGroup'}) is not None:\n",
    "                if \"Lot Size\" in item.find('span',{'class' : 'featureGroup'}).text:\n",
    "                    details['Lot Size'] = item.find('span',{'class' : 'featureName'}).text\n",
    "\n",
    "        properties.append(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eporting data into csv format\n",
    "\n",
    "df.to_csv(\"C:/Users/Karthik/Documents/Output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check whether is exported or not\n",
    "\n",
    "pd.read_csv(\"C:/Users/Karthik/Documents/Output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
